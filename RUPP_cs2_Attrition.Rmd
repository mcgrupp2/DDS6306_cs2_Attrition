---
title: "Factors Leading to Employee Attrition"
author: "Jason Rupp"
subtitle: "DDS_6306"
date: "6 Dec 19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Introduction

DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 100 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics is planning to leverage data science for talent management. The executive leadership has identified predicting employee turnover as its first application of data science for talent management. Before the business green lights the project, they have tasked your data science team to conduct an analysis of existing employee data. 

I have been given a dataset of employee data to analyize to identify factors that lead to attrition.  The top three factors that contribute to turnover will be identified backed up by evidence provided by analysis. In addition a model will be built to predict attrition, and a model to predict employee salary.

***

Install/Load Libraries
```{r Install Libraries, echo= FALSE}
#install.packages("tidyverse")
#install.packages("naniar")
#install.packages("caret")
#install.packages('e1071', dependencies = TRUE)
#install.packages("mlbench")
#install.packages("fastDummies")
#install.packages("xgboost")
#install.packages("mlr")
#install.packages("gridExtra")
#install.packages("corrplot")
#install.packages("reshape2")

```

```{r Load Libraries, results='hide'}

library(plyr)
library(tidyverse)
library(naniar)
library(caret)
library(e1071)
library(mlbench)
library(fastDummies)
library(xgboost)
library(class)
library(mlr)
library(gridExtra)
library(corrplot)
library(reshape2)
```


***

# Exploratory Data Analysis

***

### Load Data

```{r dataLoad}
# Read the data with attrition and salaries

raw_df <- read.csv("data/CaseStudy2-data.csv")

# Summary Stats of Dataframe

dim(raw_df)

names(raw_df)

summary(raw_df)

```


#### Subsetting raw_df into Attrition groups

```{r subsetting}

# Employees w. Attrition

attr_df <- raw_df %>% filter(Attrition == "Yes")

# Employees w.o Atrrition

nttr_df <- raw_df %>% filter(Attrition == "No")

# Redundant Group Summaries
#summary(attr_df)

#summary(nttr_df)

```

***

### Check for missing values in the dataset

```{r gg_miss_var, echo=FALSE}

gg_miss_var(raw_df)

```

- No missing values in the primary dataset.

***

### Company Data Exploration

We would like to visualize some data to get a general idea about this company. 

***

*The follow visualizations will represent the data from the primary dataset which was provided, the "Comp" datasets were not included in this EDA*

```{r, echo=FALSE}

#incomeHist <- 
raw_df %>% 
  ggplot(aes(x = MonthlyIncome, y = ..count.., color = Department, fill = Department)) + 
  theme_minimal() +
  geom_histogram(bins = 50, alpha = 0.85) + 
  labs(x = "Monthly Income", y = "Number of Employees", title = "Range of Monthly Income")



#raw_df %>% 
#  ggplot(aes(x=reorder(JobLevel, MonthlyIncome), y=MonthlyIncome)) + 
#  geom_boxplot(aes(fill=Attrition)) 
#
#
#raw_df %>% 
#  ggplot(aes(x=reorder(JobRole, MonthlyIncome), y=MonthlyIncome)) + 
#  geom_boxplot(aes(fill=Attrition)) 
#


```

***

```{r, echo=FALSE}

#levelIncomeBox <- 
raw_df %>% 
  ggplot(aes(x=reorder(JobLevel, MonthlyIncome), y=MonthlyIncome, fill = as.factor(JobLevel))) + 
  geom_boxplot() + 
  labs(x = "Job Level", y = "Monthly Income", title = "Ranges of Income by Job Level") +
  theme(legend.position = "none")

```

***

```{r, echo=FALSE}
#roleIncomeBox <- 
raw_df %>% 
  ggplot(aes(x=reorder(JobRole, MonthlyIncome), y=MonthlyIncome, fill = JobRole)) + 
  geom_boxplot()+ 
  labs(x = "Job Role", y = "Monthly Income", title = "Ranges of Income by Job Roe") + 
  theme(axis.text.x = element_text(angle=15, hjust = 1, size = 7), legend.position = "none")

```

***



```{r dataPrep}

set.seed(22)

fs_df <- cbind(raw_df[,2], raw_df[,4:36])

# Create dummy columns for categorical data

fs_df <- dummy_cols(fs_df)

# Remove categorical cols, replaced with dummies

f_df <- cbind(fs_df[,1],
              fs_df[,3],
              fs_df[,5:6],   
              fs_df[,10],    
              fs_df[,12],
              fs_df[,13:14], 
              fs_df[,16],    
              fs_df[,18:20], 
              fs_df[,23:25],
              fs_df[,27:60],
              fs_df[,62:63]) 

# Fix names

names(f_df)[[1]] <- "Age"
names(f_df)[[2]] <- "DailyRate"
names(f_df)[[5]] <- "EnvironmentSatisfaction"
names(f_df)[[6]] <-  "HourlyRate"
names(f_df)[[9]] <- "JobSatisfaction"

```

***

```{r featureScaling}

#~~~~~~~~~~~~~~~~Scaling features

cola <- c("DailyRate", "DistanceFromHome", "HourlyRate", "MonthlyIncome", "MonthlyRate",      
          "PercentSalaryHike", "TotalWorkingYears", "YearsAtCompany", "YearsInCurrentRole", 
          "YearsSinceLastPromotion", "YearsWithCurrManager")

all_scaled_df <- f_df %>% mutate_at(vars(cola), list(~scale(.) %>% as.vector))



```

***

```{r Remove Redundant Features + Corrplot}

#~~~~~~~~~~~~~~~~~~~~~~~Remove Redundant Features + Corrplot

correlationMatrix <- cor(all_scaled_df, use="pairwise.complete.obs")

corrplot(correlationMatrix, tl.cex = 0.5)

# Search for the highly correlated features to remove for ML models

highlyCorrelated <- findCorrelation(correlationMatrix, cutoff = 0.75)

f_df <- all_scaled_df[,-(highlyCorrelated)]

```

***

```{r featureSelection}

#~~~~~~~~~~~~~~~~~~~~Feature Selection (Automatic)------LONG LOAD

control <- rfeControl(functions=rfFuncs, method="cv", number=10)

results <- rfe(f_df, raw_df[,3], sizes=c(1:42), rfeControl=control)

p_inx <- predictors(results)

pd_df <- all_scaled_df[,p_inx]

plot(results, type=c("g", "o"))

```

***

```{r knn model Attrition}

set.seed(22)
iterations = 500
numks = 20
splitPerc = .7
masterAcc = matrix(nrow = iterations, ncol = numks)
masterSens = matrix(nrow = iterations, ncol = numks)
masterSpec = matrix(nrow = iterations, ncol = numks)
for(j in 1:iterations)
{
  k_inx = sample(1:dim(pd_df)[1],round(splitPerc * dim(pd_df)[1]))
  
  k_trn_data <- pd_df[k_inx,]
  
  k_tst_data <- pd_df[-k_inx,]
  
  #cl = raw_df[k_inx, 3]
  
  for(i in 1:numks)
  {
    classifications = knn(k_trn_data, 
                          k_tst_data, 
                          raw_df[k_inx, 3], 
                          prob = TRUE, k = i)
    table(classifications,raw_df[-k_inx, 3])
    CM = confusionMatrix(table(classifications, raw_df[-k_inx, 3]))
    masterAcc[j,i] = CM$overall[1]
    masterSens[j,i] = CM$byClass[1]
    masterSpec[j,i] = CM$byClass[2]
  }
  
}

MeanAcc = colMeans(masterAcc)
plot(seq(1,numks,1),MeanAcc, type = "l", main = "Overall Accuracy", xlab = "Number of k(s)", ylab = "Mean Accuracy")
which.max(MeanAcc)
max(MeanAcc)

MeanSens = colMeans(masterSens)
plot(seq(1,numks,1),MeanSens, type = "l", main = "Overall Sensitivy", xlab = "Number of k(s)", ylab = "Mean Sensitivy")
which.max(MeanSens)
max(MeanSens)

MeanSpec = colMeans(masterSpec)
plot(seq(1,numks,1),MeanSpec, type = "l", main = "Overall Specificity", xlab = "Number of k(s)", ylab = "Mean Specificity")
which.max(MeanSpec)
max(MeanSpec)



```


```{r, echo=FALSE}



```
```{r, echo=FALSE}



```
```{r, echo=FALSE}



```
```{r, echo=FALSE}



```
```{r, echo=FALSE}



```
```{r, echo=FALSE}



```